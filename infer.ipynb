{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "from distutils.command import check\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import mmcv\n",
    "from os import path as op\n",
    "\n",
    "from mmcls.apis import inference_model, init_model, show_result_pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_dir=\"\" #图片的路径 \n",
    "# device='cuda:0'  # cuda:0 or cuda:1,2,3...，指定你的gpu\n",
    "# #注意使用模型集成时需考虑config和checkpoint为list\n",
    "# config=\"\" #和training config保持一致，str or list\n",
    "# checkpoint=\"\" #checkpoint path，模型训练结果的路径 str or list\n",
    "# csv_result=\"\" #the path to save csv file，用于保存csv结果的路径\n",
    "# pu_label=False # ture or false，选择是否使用伪标签技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir=\"data/led/test\" #图片的路径 \n",
    "device='cuda:0'  # cuda:0 or cuda:1,2,3...，指定你的gpu\n",
    "#注意使用模型集成时需考虑config和checkpoint为list\n",
    "config=\"configs/xunfei2022led/resnet34_1xb32_led.py\" #和training config保持一致，str or list\n",
    "checkpoint=\"training/resnet50_1xb32_led/epoch_50.pth\" #checkpoint path，模型训练结果的路径 str or list\n",
    "csv_result=\"resnet34_1xb32_led.csv\" #the path to save csv file，用于保存csv结果的路径\n",
    "pu_label=False # ture or false，选择是否使用伪标签技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: training/resnet50_1xb32_led/epoch_50.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for backbone.layer1.0.conv1.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n",
      "size mismatch for backbone.layer1.1.conv1.weight: copying a param with shape torch.Size([64, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n",
      "size mismatch for backbone.layer1.2.conv1.weight: copying a param with shape torch.Size([64, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n",
      "size mismatch for backbone.layer2.0.conv1.weight: copying a param with shape torch.Size([128, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n",
      "size mismatch for backbone.layer2.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n",
      "size mismatch for backbone.layer2.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "size mismatch for backbone.layer2.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "size mismatch for backbone.layer2.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "size mismatch for backbone.layer2.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([128]).\n",
      "size mismatch for backbone.layer2.1.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n",
      "size mismatch for backbone.layer2.2.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n",
      "size mismatch for backbone.layer2.3.conv1.weight: copying a param with shape torch.Size([128, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n",
      "size mismatch for backbone.layer3.0.conv1.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n",
      "size mismatch for backbone.layer3.0.downsample.0.weight: copying a param with shape torch.Size([1024, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n",
      "size mismatch for backbone.layer3.0.downsample.1.weight: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n",
      "size mismatch for backbone.layer3.0.downsample.1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n",
      "size mismatch for backbone.layer3.0.downsample.1.running_mean: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n",
      "size mismatch for backbone.layer3.0.downsample.1.running_var: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([256]).\n",
      "size mismatch for backbone.layer3.1.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n",
      "size mismatch for backbone.layer3.2.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n",
      "size mismatch for backbone.layer3.3.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n",
      "size mismatch for backbone.layer3.4.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n",
      "size mismatch for backbone.layer3.5.conv1.weight: copying a param with shape torch.Size([256, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n",
      "size mismatch for backbone.layer4.0.conv1.weight: copying a param with shape torch.Size([512, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 3, 3]).\n",
      "size mismatch for backbone.layer4.0.downsample.0.weight: copying a param with shape torch.Size([2048, 1024, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n",
      "size mismatch for backbone.layer4.0.downsample.1.weight: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n",
      "size mismatch for backbone.layer4.0.downsample.1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n",
      "size mismatch for backbone.layer4.0.downsample.1.running_mean: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n",
      "size mismatch for backbone.layer4.0.downsample.1.running_var: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([512]).\n",
      "size mismatch for backbone.layer4.1.conv1.weight: copying a param with shape torch.Size([512, 2048, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n",
      "size mismatch for backbone.layer4.2.conv1.weight: copying a param with shape torch.Size([512, 2048, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 512, 3, 3]).\n",
      "size mismatch for head.fc.weight: copying a param with shape torch.Size([2, 2048]) from checkpoint, the shape in current model is torch.Size([2, 512]).\n",
      "unexpected key in source state_dict: backbone.layer1.0.conv3.weight, backbone.layer1.0.bn3.weight, backbone.layer1.0.bn3.bias, backbone.layer1.0.bn3.running_mean, backbone.layer1.0.bn3.running_var, backbone.layer1.0.bn3.num_batches_tracked, backbone.layer1.0.downsample.0.weight, backbone.layer1.0.downsample.1.weight, backbone.layer1.0.downsample.1.bias, backbone.layer1.0.downsample.1.running_mean, backbone.layer1.0.downsample.1.running_var, backbone.layer1.0.downsample.1.num_batches_tracked, backbone.layer1.1.conv3.weight, backbone.layer1.1.bn3.weight, backbone.layer1.1.bn3.bias, backbone.layer1.1.bn3.running_mean, backbone.layer1.1.bn3.running_var, backbone.layer1.1.bn3.num_batches_tracked, backbone.layer1.2.conv3.weight, backbone.layer1.2.bn3.weight, backbone.layer1.2.bn3.bias, backbone.layer1.2.bn3.running_mean, backbone.layer1.2.bn3.running_var, backbone.layer1.2.bn3.num_batches_tracked, backbone.layer2.0.conv3.weight, backbone.layer2.0.bn3.weight, backbone.layer2.0.bn3.bias, backbone.layer2.0.bn3.running_mean, backbone.layer2.0.bn3.running_var, backbone.layer2.0.bn3.num_batches_tracked, backbone.layer2.1.conv3.weight, backbone.layer2.1.bn3.weight, backbone.layer2.1.bn3.bias, backbone.layer2.1.bn3.running_mean, backbone.layer2.1.bn3.running_var, backbone.layer2.1.bn3.num_batches_tracked, backbone.layer2.2.conv3.weight, backbone.layer2.2.bn3.weight, backbone.layer2.2.bn3.bias, backbone.layer2.2.bn3.running_mean, backbone.layer2.2.bn3.running_var, backbone.layer2.2.bn3.num_batches_tracked, backbone.layer2.3.conv3.weight, backbone.layer2.3.bn3.weight, backbone.layer2.3.bn3.bias, backbone.layer2.3.bn3.running_mean, backbone.layer2.3.bn3.running_var, backbone.layer2.3.bn3.num_batches_tracked, backbone.layer3.0.conv3.weight, backbone.layer3.0.bn3.weight, backbone.layer3.0.bn3.bias, backbone.layer3.0.bn3.running_mean, backbone.layer3.0.bn3.running_var, backbone.layer3.0.bn3.num_batches_tracked, backbone.layer3.1.conv3.weight, backbone.layer3.1.bn3.weight, backbone.layer3.1.bn3.bias, backbone.layer3.1.bn3.running_mean, backbone.layer3.1.bn3.running_var, backbone.layer3.1.bn3.num_batches_tracked, backbone.layer3.2.conv3.weight, backbone.layer3.2.bn3.weight, backbone.layer3.2.bn3.bias, backbone.layer3.2.bn3.running_mean, backbone.layer3.2.bn3.running_var, backbone.layer3.2.bn3.num_batches_tracked, backbone.layer3.3.conv3.weight, backbone.layer3.3.bn3.weight, backbone.layer3.3.bn3.bias, backbone.layer3.3.bn3.running_mean, backbone.layer3.3.bn3.running_var, backbone.layer3.3.bn3.num_batches_tracked, backbone.layer3.4.conv3.weight, backbone.layer3.4.bn3.weight, backbone.layer3.4.bn3.bias, backbone.layer3.4.bn3.running_mean, backbone.layer3.4.bn3.running_var, backbone.layer3.4.bn3.num_batches_tracked, backbone.layer3.5.conv3.weight, backbone.layer3.5.bn3.weight, backbone.layer3.5.bn3.bias, backbone.layer3.5.bn3.running_mean, backbone.layer3.5.bn3.running_var, backbone.layer3.5.bn3.num_batches_tracked, backbone.layer4.0.conv3.weight, backbone.layer4.0.bn3.weight, backbone.layer4.0.bn3.bias, backbone.layer4.0.bn3.running_mean, backbone.layer4.0.bn3.running_var, backbone.layer4.0.bn3.num_batches_tracked, backbone.layer4.1.conv3.weight, backbone.layer4.1.bn3.weight, backbone.layer4.1.bn3.bias, backbone.layer4.1.bn3.running_mean, backbone.layer4.1.bn3.running_var, backbone.layer4.1.bn3.num_batches_tracked, backbone.layer4.2.conv3.weight, backbone.layer4.2.bn3.weight, backbone.layer4.2.bn3.bias, backbone.layer4.2.bn3.running_mean, backbone.layer4.2.bn3.running_var, backbone.layer4.2.bn3.num_batches_tracked\n",
      "\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>] 1494/1494, 29.5 task/s, elapsed: 51s, ETA:     0s\n"
     ]
    }
   ],
   "source": [
    "# 注意，这里是模型集成的部分代码，假如你要使用模型集成，这里并未完全实现，期待您的实现\n",
    "if isinstance(config, list) and isinstance(checkpoint, list):  # 这一行代码有误 args.config， args.checkpoint 需要你自行修改\n",
    "    for config, checkpoint in zip(config, checkpoint):\n",
    "        models = init_model(config, checkpoint, device=device) \n",
    "else:\n",
    "    # build the model from a config file and a checkpoint file\n",
    "    models = init_model(config, checkpoint, device=device)    \n",
    "\n",
    "\n",
    "# write to the csv file\n",
    "headers = ['image', 'label']\n",
    "with open(csv_result, 'w+', newline=\"\") as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    f_csv.writerow(headers)\n",
    "\n",
    "    # test  images\n",
    "    for img in mmcv.track_iter_progress(os.listdir(img_dir)):\n",
    "        _img = op.join(img_dir, img)\n",
    "        results = []\n",
    "        # 当存在多个模型时\n",
    "        if isinstance(models, list):\n",
    "            for model in models:\n",
    "                result = inference_model(model, _img)\n",
    "                results.append(result)#模型集成，未完成\n",
    "        else:\n",
    "            # 当仅为单模型时\n",
    "            result = inference_model(models, _img)\n",
    "            \n",
    "        # 模型集成，伪标签技术（是比赛中常用的很有效的涨点方式），这里我们用pred_score分数去做(PS: 未完整实现，大家如果感兴趣，可以去实现它们)\n",
    "        pred_label, pred_score, pred_class = result['pred_label'], result['pred_score'], result['pred_class']  \n",
    "        # write  rows\n",
    "        \n",
    "        # 本次比赛为分类问题：这里我们设定一个足够高的阈值, 选择预测概率最有把握的样本的标签作为真实的标签（例如概率为0.99或者概率未0.01的预测标签\n",
    "        # 将预测然后将得到的有标注的数据加入原始数据继续进行训练，再预测，一直到达停止条件（例如大部分甚至全部unlabeled的样本都被打上了标签），\n",
    "        # 此时，我们就把未标注的样本通过这种方式标注出来了\n",
    "        # 这里阈值可以根据实际情况自行设定\n",
    "        if pu_label == True:\n",
    "            if pred_score > 0.9:\n",
    "                row = [img, pred_label]\n",
    "                f_csv.writerow(row)\n",
    "        else:\n",
    "            row = [img, pred_label]\n",
    "            # import pdb;pdb.set_trace()\n",
    "            f_csv.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('datawhale2022')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "880e36c64ab5337b7c67e7c3147d2af7a040cac424175a8d311ece8a163d4623"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
